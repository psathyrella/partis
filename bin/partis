#!/usr/bin/env python
from distutils.version import StrictVersion
import argparse
import copy
import time
import random
import sys
import subprocess
import multiprocessing
import numpy
import scipy
if StrictVersion(scipy.__version__) < StrictVersion('0.17.0'):
    raise RuntimeError("scipy version 0.17 or later is required (found version %s)." % scipy.__version__)
import colored_traceback.always
import os
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')

import utils
import glutils
import treeutils
import processargs
import seqfileopener
from partitiondriver import PartitionDriver

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    from recombinator import Recombinator

    utils.prep_dir(args.workdir)

    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is not None:  # the command line explicitly told us where to get the glfo
        if args.reco_parameter_dir is not None:
            print '  note: getting germline sets from --initial-germline-dir, even though --reco-parameter-dir was also set'
        input_gldir = args.initial_germline_dir
    else:  # if it wasn't explicitly set, we have to decide what to use
        if args.rearrange_from_scratch:  # just use the default
            input_gldir = args.default_initial_germline_dir
        else:  # otherwise assume we're supposed to use the glfo in --reco-parameter-dir
            input_gldir = args.reco_parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
    glfo = glutils.read_glfo(input_gldir, args.locus, only_genes=args.only_genes)
    if not args.im_a_subproc and args.rearrange_from_scratch and args.generate_germline_set:
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_sim_alleles_per_gene, args.min_sim_allele_prevalence_freq, args.allele_prevalence_fname, new_allele_info=args.new_allele_info, debug=args.debug)  # NOTE removes unwanted genes from <glfo>
        if utils.getsuffix(args.outfname) == '.csv':
            print '  writing generated germline set to %s/' % args.outfname.replace('.csv', '-glfo')
            glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
    if args.subsimproc:
        working_gldir = args.workdir + '/' + glutils.glfo_dir
        glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def get_subproc_cmd_str(n_events, iproc, workdir, outfname):
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
        utils.remove_from_arglist(clist, '--subsimproc')
        clist.append('--im-a-subproc')
        utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
        utils.replace_in_arglist(clist, '--workdir', workdir)
        utils.replace_in_arglist(clist, '--outfname', outfname)
        utils.replace_in_arglist(clist, '--n-sim-events', str(n_events))
        utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
        if args.allele_prevalence_fname is not None:
            utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)
        return ' '.join(clist)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, iproc, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed+iproc, workdir=workdir)
        events = []
        for ievt in range(n_events):
            event = reco.combine(random_ints[ievt])
            events.append(event)
        utils.write_annotations(outfname, glfo, events, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns), synth_single_seqs=utils.getsuffix(outfname) == '.csv', use_pyyaml=args.write_full_yaml_output)  # keep writing the csv as single-sequence lines, just for backwards compatibility (now trying to switch to synthesizing single seq lines when reading) NOTE list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.

    def get_workdir(iproc):
        return args.workdir + '/sub-' + str(iproc)
    def get_outfname(iproc):
        return get_workdir(iproc) + '/' + os.path.basename(args.outfname)

    if not args.im_a_subproc:
        print 'simulating'

    n_per_proc = int(float(args.n_sim_events) / args.n_procs)

    # generate all the random seeds NOTE these aren't used if <args.subsimproc> is set, i.e. results will be different with and without <args.subsimproc> set.
    all_random_ints = []
    for iproc in range(args.n_procs):  # have to do it all at once, 'cause each of the subprocesses is going to reset its seed and god knows what happens to our seed at that point
        all_random_ints.append([random.randint(0, numpy.iinfo(numpy.int32).max) for i in range(n_per_proc)])

    if args.n_procs == 1:  # multiprocessing is kind of messy
        make_events(n_per_proc, 0, args.workdir, args.outfname, all_random_ints[0])
    else:  # start the processes and wait for 'em to finish
        cmdfos = [{'cmd_str' : get_subproc_cmd_str(n_per_proc, iproc, get_workdir(iproc), get_outfname(iproc)) if args.subsimproc else None,
                   'workdir' : get_workdir(iproc),
                   'logdir' : args.workdir + '/log-' + str(iproc),  # have to be different than <workdirs> since ./bin/partis barfs if its workdir already exists (as it should)
                   'outfname' : get_outfname(iproc)}
                  for iproc in range(args.n_procs)]
        if args.subsimproc:
            utils.run_cmds(cmdfos, batch_system=args.batch_system, batch_options=args.batch_options, batch_config_fname=args.batch_config_fname, debug='print')
        else:
            for iproc in range(args.n_procs):
                proc = multiprocessing.Process(target=make_events, args=(n_per_proc, iproc, cmdfos[iproc]['workdir'], cmdfos[iproc]['outfname'], all_random_ints[iproc]))
                proc.start()
            while len(multiprocessing.active_children()) > 0:
                sys.stdout.flush()
                time.sleep(1)

        file_list = [cmdfos[i]['outfname'] for i in range(args.n_procs)]
        utils.merge_simulation_files(args.outfname, file_list, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns), n_total_expected=args.n_sim_events, n_per_proc_expected=n_per_proc, use_pyyaml=args.write_full_yaml_output)  # list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v', debug=args.debug)
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if args.subsimproc:
        glutils.remove_glfo_files(working_gldir, args.locus)
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])

    if not args.im_a_subproc:
        try:
            if args.outfname == processargs.get_dummy_outfname(args.workdir) and os.path.exists(processargs.get_dummy_outfname(args.workdir)):
                os.remove(processargs.get_dummy_outfname(args.workdir))
            os.rmdir(args.workdir)
        except OSError:
            raise Exception('workdir (%s) not empty: %s' % (args.workdir, ' '.join(os.listdir(args.workdir))))  # hm... you get weird recursive exceptions if you get here. Oh, well, it still works

# ----------------------------------------------------------------------------------------
def read_inputs(args, actions):
    if actions[0] == 'cache-parameters':  # for parameter caching, use the default in data/germlines/human unless something else was set on the command line
        gldir = args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir
    elif runs_on_existing_output(actions[0]):
        if utils.getsuffix(args.outfname) == '.csv':  # old way
            if args.initial_germline_dir is not None:
                gldir = args.initial_germline_dir
            elif args.parameter_dir is not None and os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
                gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
            else:
                raise Exception('couldn\'t guess germline info location with deprecated .csv output file: either set it with --intitial-germline-dir or --parameter-dir, or use .yaml output files so germline info is written to the same file as the rest of the output')
        elif utils.getsuffix(args.outfname) == '.yaml':  # new way
            gldir = None  # gets set when we read the glfo from the yaml in partitiondriver
        else:
            raise Exception('unhandled annotation file suffix %s' % args.outfname)
    else:
        if args.initial_germline_dir is not None:
            gldir = args.initial_germline_dir
        elif args.parameter_dir is not None and os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
            gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
        else:
            tstr = ''
            if args.parameter_dir is not None and not os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
                tstr = ' (--parameter-dir was set to %s, but the corresponding glfo dir %s doesn\'t exist)' % (args.parameter_dir, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
            raise Exception('couldn\'t guess germline info location: set it with either --intitial-germline-dir or --parameter-dir%s' % tstr)

    glfo = None
    if gldir is not None:
        template_glfo = None
        if args.sanitize_input_germlines:
            print '   using default germline dir %s for template glfo (e.g. for codon positions)' % args.default_initial_germline_dir
            template_glfo = glutils.read_glfo(args.default_initial_germline_dir, args.locus)
        glfo = glutils.read_glfo(gldir, args.locus, only_genes=args.only_genes, template_glfo=template_glfo, add_dummy_name_components=args.sanitize_input_germlines, debug=2 if args.sanitize_input_germlines else False)

    simglfo = None
    if not args.is_data:
        if args.infname is not None and utils.getsuffix(args.infname) == '.yaml':
            simglfo = None  # cause we'll read it from the input file
        elif args.simulation_germline_dir is not None:  # if an explicit dir was set on the command line
            simglfo = glutils.read_glfo(args.simulation_germline_dir, locus=args.locus)  # probably don't apply <args.only_genes> (?)
        else:
            raise Exception('couldn\'t find simulation germline info: either set with --simulation-germline-dir, or use .yaml simulation files and set --infname')

    more_input_info = None
    if args.queries_to_include_fname is not None:
        more_input_info, _, _ = seqfileopener.read_sequence_file(args.queries_to_include_fname, args.is_data)
        print '  --queries-to-include-fname: adding %d extra sequence%s from %s' % (len(more_input_info), utils.plural(len(more_input_info)), args.queries_to_include_fname)
        if args.queries_to_include is None:
            args.queries_to_include = []
        args.queries_to_include += more_input_info.keys()

    if args.infname is None:  # put this *after* setting queries_to_include from a file, since we need that set
        return None, None, glfo, simglfo

    input_info, reco_info, yaml_glfo = seqfileopener.read_sequence_file(args.infname, args.is_data, n_max_queries=args.n_max_queries, args=args, simglfo=simglfo, more_input_info=more_input_info)
    if not args.is_data and yaml_glfo is not None:  # NOTE is is extremely important that <glfo> doesn't get set to the true info in a simulation yaml
        simglfo = yaml_glfo

    if len(input_info) > 1000 and args.n_procs == 1:
        print '  note: running on %d sequences with only %d process%s. This may be kinda slow, so it might be a good idea to set --n-procs N to the number of processors on your local machine, or look into non-local parallelization with --batch-system.' % (len(input_info), args.n_procs, utils.plural(args.n_procs, prefix='e'))
    if len(input_info) > 1000 and args.outfname is None:
        print '  note: running on a lot of sequences (%d) without setting --outfname. Which is ok, but there will be no persistent record of the results (except the parameter directory).' % len(input_info)
    return input_info, reco_info, glfo, simglfo

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    if args.parameter_dir is None:
        args.parameter_dir = '_output/' + args.infname[ : args.infname.rfind('.')].replace('/', '_') if args.infname is not None else 'xxx-dummy-xxx'  # the latter is a shitty convention, but I code further on crashes if I let the parameter dir be None

    actions = [args.action]  # do *not* use <args.action> after this (well, for anything other than checking what was actually set on the command line)
    if args.action in ['annotate', 'partition'] and not os.path.exists(args.parameter_dir):
        actions = ['cache-parameters'] + actions
        print '  parameter dir \'%s\' does not exist, so caching a new set of parameters before running action \'%s\'' % (args.parameter_dir, actions[1])
        if args.seed_unique_id is not None or args.seed_seq is not None:  # if we're auto parameter caching for/before seed partitioning, we *don't* (yet) want to remove non-clonal sequences, since we need all the non-clonal sequences to get better parameters (maybe at some point we want to be able to count parameters just on this lineage, but for now let's keep it simple)
            raise Exception('if setting --seed-unique-id or --seed-seq for \'partition\', you must first explicitly run \'cache-parameters\' in order to ensure that parameters are cached on all sequences, not just clonally related sequences.')

    input_info, reco_info, glfo, simglfo = read_inputs(args, actions)
    parter = PartitionDriver(args, glfo, input_info, simglfo, reco_info)
    parter.run(actions)
    if not runs_on_existing_output(args.action):  # mostly wanted to avoid rewriting the persistent hmm cache file
        parter.clean()

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--locus', default='igh', choices=utils.loci.keys(), help='which immunoglobulin or t-cell receptor locus?')
parent_parser.add_argument('--loci', help='which immunoglobulin or t-cell receptor loci?')
parent_parser.add_argument('--chain', choices=('h', 'k', 'l'), help='DEPRECATED in favor of --locus, exists only for backwards compatibility')
parent_parser.add_argument('--species', default='human', choices=('human', 'macaque', 'mouse'), help='Which species?')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which to restrict the analysis')
parent_parser.add_argument('--queries-to-include', help='When reading the input file, look for and include these additional uids when --n-random-queries is set (*not* compatible with --n-max-queries). Contrast with --queries, which includes *only* the indicated uids. Additionally, these queries are treated differently e.g. when partition plotting or removing duplicate sequences.')
parent_parser.add_argument('--queries-to-include-fname', help='In cases where you want certain sequences to be included in --queries-to-include or --seed-unique-id, but these sequences are not in --infname, you can put them in this file. Typically, this is useful when you have a number of seed sequences that are from a separate experiment than the NGS data in --infname.')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences to read from input file, starting from beginning of file')
parent_parser.add_argument('--n-random-queries', type=int, help='choose this many queries at random from entire input file')
parent_parser.add_argument('--istartstop', help='colon-separated start:stop line indices for input sequence file (with python slice conventions, e.g. if set to \'2:4\' will skip the zeroth and first sequences, and then take the following two sequences, and then skip all subsequence sequences). Applied before any other input filters, e.g. --n-max-queries, --queries, --reco-ids, etc.')

parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')
parent_parser.add_argument('--only-print-best-partition', action='store_true', help='when printing cluster annotations (either with --debug 1 for \'partition\', or for \'view-output\'), instead of the default of printing the annotation for every cluster for which we calculated one (see --calculate-alternative-naive-seqs and --n-partitions-to-write), only print annotations for clusters in the best partition.')
parent_parser.add_argument('--only-print-seed-clusters', action='store_true', help='same as --only-print-best-partition, but in addition, only print the seed cluster(s). Note that if --only-print-best-partition is *not* set, then there will be more than one seed cluster.')

parent_parser.add_argument('--n-procs', type=int, default=1, help='Number of processes over which to parallelize. This is usually the maximum that will be initialized at any given time, but for internal reasons, certain steps (e.g. smith waterman and partition naive sequence precaching) sometimes use slightly more.')
parent_parser.add_argument('--n-max-to-calc-per-process', default=250, help='if a bcrham process calc\'d more than this many fwd + vtb values (and this is the first time with this number of procs), don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--min-hmm-step-time', default=2., help='if a clustering step takes fewer than this many seconds, always reduce n_procs')
parent_parser.add_argument('--batch-system', choices=['slurm', 'sge'], help='batch system with which to attempt paralellization')
parent_parser.add_argument('--batch-options', help='additional options to apply to --batch-system (e.g. --batch-options="--foo bar")')
parent_parser.add_argument('--batch-config-fname', default='/etc/slurm-llnl/slurm.conf', help='system-wide batch system configuration file name')  # for when you're running the whole thing within one slurm allocation, i.e. with  % salloc --nodes N ./bin/partis [...]

parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
parent_parser.add_argument('--count-parameters', action='store_true', help='force parameter counting when action is not cache-parameters (presumably so that you can plot them)')
parent_parser.add_argument('--dont-write-parameters', action='store_true', help='don\'t write parameters to disk even if you\'ve counted them (mostly for use in conjunction with --only-smith-waterman, so you can avoid cluttering up your file system)')
parent_parser.add_argument('--write-trimmed-and-padded-seqs-to-sw-cachefname', action='store_true', help='after running sw, trim and pad sequences *before* writing to the sw cache file (rather than after). Note that this will in some cases cause waterer.py to not be able to read sw results from this cache file.')
parent_parser.add_argument('--partis-dir', default=partis_dir, help='for internal use only')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--vsearch-binary',  help='Path to vsearch binary (vsearch binaries for linux and darwin are pre-installed in bin/, but for other systems you need to get your own)')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (i.e. if they have stop codons, out of frame cdr3, or mutated cyst/tryp/phen)')
parent_parser.add_argument('--also-remove-duplicate-sequences-with-different-lengths', action='store_true', help='By default we remove any queries which have exactly the same sequence as a previous query. If this is set, we also consider as duplicates sequences which are sub/super strings of previous sequences (we keep the longest one).')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='By default we trim anything to the 5\' of V and 3\' of J in order to remove queries with identical coding regions. This turns that off.')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--no-indels', action='store_true', help='Tell smith-waterman not to look for indels, by drastically increasing the gap-open penalty (you can also set the penalty directly).')
parent_parser.add_argument('--no-indel-gap-open-penalty', type=int, default=1000, help='set --gap-open-penalty to this when --no-indels is set (also used in python/waterer.py')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed for use (mostly) by recombinator (to allow reproducibility)')
parent_parser.add_argument('--min-observations-per-gene', type=int, default=20, help='If a gene is observed fewer times than this, we average over other alleles/primary versions/etc. (e.g. in recombinator and hmmwriter). Also used as a more general "this isn\'t very many observations" value.')
parent_parser.add_argument('--no-per-base-mfreqs', action='store_true')
parent_parser.add_argument('--region-end-exclusion-length', type=int, default=0, help='when counting/writing parameters, ignore this many bases abutting non-templated insertions for calculating mutation frequencies (note: doesn\'t make a difference (hence set to 0 by default) probably because we\'re setting a strongish prior on these bases when writing hmms anyway')

parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--max-vj-mut-freq', type=float, default=0.4, help='skip sequences whose mutation rates in V and J are greater than this (it\'s really not possible to get meaningful smith-waterman matches above this)')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-simultaneous-seqs', type=int, help='Number of simultaneous sequences on which to run the multi-HMM (e.g. 2 for a pair hmm)')
parent_parser.add_argument('--all-seqs-simultaneous', action='store_true', help='Run all input sequences simultaneously, i.e. equivalent to setting --n-simultaneous-seqs to the number of input sequences.')
parent_parser.add_argument('--simultaneous-true-clonal-seqs', action='store_true', help='Run true clonal sequences together simultaneously with the multi-HMM.')

parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq, .csv, or partis output .yaml (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--name-column', help='column/key name for sequence ids in input csv/yaml file (default: \'unique_ids\')')
parent_parser.add_argument('--seq-column', help='column/key name for nucleotide sequences in input csv/yaml file (default: \'input_seqs\')')
parent_parser.add_argument('--input-metafname', help='yaml file with meta information for the sequences in --infname (and --queries-to-include-fname), keyed by sequence id. Currently accepted keys/columns are \'timepoint\', \'affinity\', and \'multiplicity\'.')
parent_parser.add_argument('--outfname', help='output file name')
parent_parser.add_argument('--write-full-yaml-output', action='store_true', help='By default, we write yaml output files using the json subset of yaml, since it\'s much faster. If this is set, we instead write full yaml, which is more human-readable (but also much slower).')
parent_parser.add_argument('--presto-output', action='store_true', help='Write output file(s) in presto/changeo format. Since this format depends on a particular IMGT alignment, you must also pass a fasta file with alignments for all the V, D, and J germline genes using --aligned-germline-fname (an example file, which may or may not have all the genes you need, is at data/germlines/imgt-aligned-igh.fa). For the \'annotate\' action, a single .tsv file is written with annotations (so --outfname suffix must be .tsv). For the \'partition\' action, a fasta file is written with cluster information (so --outfname suffix must be .fa or .fasta), as well as a .tsv in the same directory with the corresponding annotations.')
parent_parser.add_argument('--extra-annotation-columns', help='Extra columns to add to the (fairly minimal) set of information written by default to annotation output files (choose from: %s' % ' '.join(utils.extra_annotation_headers))  # NOTE '-columns' in command line arg, but '-headers' in utils (it's more consistent that way, I swear)
parent_parser.add_argument('--linearham', action='store_true', help='write Smith-Waterman flexbounds/relpos values in linearham format to output file')
parent_parser.add_argument('--cluster-annotation-fname', help='output file for cluster annotations (default is <--outfname>-cluster-annotations.<suffix>)')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--parameter-type', default='hmm', choices=('sw', 'hmm'), help='Use parameters from Smith-Waterman (sw) or the HMM (hmm) subdirectories for inference/simulation? (you should almost certainly use the hmm ones, but sw is occasionally useful for debugging)')
parent_parser.add_argument('--parameter-out-dir', help='Special parameter dir for writing multi-hmm parameters, i.e. when running annotate or partition with --count-parameters set (if not set, defaults to <--parameter-dir>/multi-hmm).')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--write-sw-cachefile', action='store_true', help='Write sw results to the sw cache file during actions for which we\'d normally only look for an existing one (i.e annotate and partition).')
parent_parser.add_argument('--workdir', help='Temporary working directory (default is set below)')

parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (by default this is not set, and consequently no plots are written')
parent_parser.add_argument('--plot-performance', action='store_true', help='renamed to --plot-annotation-performance')
parent_parser.add_argument('--plot-annotation-performance', action='store_true', help='Write out plots comparing true and inferred annotation accuracy distributions')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='only write csv plots (svg writing is pretty slow, and it\'s frequently better to make comparison plots from csvs with ./bin/compare-plotdirs.py anyway)')
parent_parser.add_argument('--make-per-gene-plots', action='store_true', help='in addition to plots aggregating over genes, write plots displaying info for each gene of, e.g., per position shm rate, deletion frequencies')
parent_parser.add_argument('--ete-path', default=('/home/%s/anaconda_ete/bin' % os.getenv('USER')) if os.getenv('USER') is not None else None)

parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='For internal use only. To specify your own germline directory from which to start, use --initial-germline-dir instead.')
parent_parser.add_argument('--initial-germline-dir', help='Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse')
parent_parser.add_argument('--sanitize-input-germlines', action='store_true', help='By default we require all gene names to look like IGHVx*y. If you set this option when you\'re also setting --initial-germline-dir, then we instead allow arbitrary strings as gene names in this input germline directory, and we sanitize them by adding the correct locus and region info at the start (so make sure that you have --locus set correctly), and adding an arbitrary allele string at the end (e.g. <stuff> --> IGHV<stuff>*x).')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (used if --is-simu is set, altough if the default germline info is similar to that used for simulation it may not be necessary)')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with alignments for each V gene (if using with --presto-output, must contain an alignment also for all D and J genes). An example is located in data/germlines/imgt-aligned-igh.fa')

parent_parser.add_argument('--n-max-alleles-per-gene', type=int, default=None, help='if set, after allele inference the germline set is reduced such that each imgt-name-defined gene has at most this many alleles, with the alleles assigned to the fewest sequences removed first.')
parent_parser.add_argument('--typical-genes-per-region-per-subject', default='55:25:6', help='typical number of alleles per subject for the v, d, and j regions (used by --min-allele-prevalence-fraction)')
parent_parser.add_argument('--min-allele-prevalence-fraction', type=float, default=0.0005, help='Remove any V alleles that represent less than this fraction of the repertoire (rescaled using --typical-genes-per-region-per-subject for D and J). Converted to --min-allele-prevalence-fractions (note plural!) internally.')

parent_parser.add_argument('--leave-default-germline', action='store_true')
parent_parser.add_argument('--dont-remove-unlikely-alleles', action='store_true')
parent_parser.add_argument('--allele-cluster', action='store_true')
parent_parser.add_argument('--kmeans-allele-cluster', action='store_true')
parent_parser.add_argument('--dont-find-new-alleles', action='store_true')
# parent_parser.add_argument('--always-find-new-alleles', action='store_true', help='By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.')
parent_parser.add_argument('--debug-allele-finding', action='store_true', help='print lots of debug info on new-allele fits')
parent_parser.add_argument('--new-allele-fname', help='fasta fname to which to write any new alleles (they are also written, together with previously-known alleles that are also in the sample, to --parameter-dir)')
parent_parser.add_argument('--n-max-snps', type=int, default=8, help='when new-allele finding, look for new V alleles separated from existing V alleles by up to this many SNPs. Also used for allele removal (corresponding numbers for D and J are set automatically)')
parent_parser.add_argument('--n-max-mutations-per-segment', type=int, default=23, help='when new-allele finding, exclude sequences which have more than this many mutations in the V segment')
parent_parser.add_argument('--min-allele-finding-gene-length', type=int, default=150, help='if (after excluding particularly short reads) the reads for a gene are shorter than this, then don\'t look for new alleles with/on this gene')
parent_parser.add_argument('--plot-and-fit-absolutely-everything', type=int, help='fit every single position for this <istart> and write every single corresponding plot (slow as hell, and only for debugging/making plots for paper)')
parent_parser.add_argument('--cluster-indices', help='indices of clusters (when sorted largest to smallest) to print for the \'view-output\' action.')
parent_parser.add_argument('--min-tree-metric-cluster-size', type=int, default=5, help='don\'t calculate tree metrics for clusters smaller than this')
parent_parser.add_argument('--treefname', help='newick-formatted file with a tree corresponding to the sequences either in --infname (if making new output, i.e. action is annotate or partition) or --outfname (if reading existing output, i.e. action is get-tree-metrics). All sequences in the annotation must be in the tree, but the can be sequences/nodes in the tree that aren\'t in the annotation (presumably because you used something like FastTree that doesn\'t infer ancestral sequences)).')
parent_parser.add_argument('--lb-tau', type=float, default=treeutils.default_lb_tau, help='exponential decay length for local branching index/ratio calculations (0.01 corresponds to a branch length with 1%% mutation)')  # default is set in treeutils so we can import it from bin/get-tree-metrics.py

parent_parser.add_argument('--no-sw-vsearch', action='store_true', help='By default, we get preliminary V annotations from vsearch that we pass to sw. This improves accuracy because the vsearch shm rate estimates let us separate sequences into groups by optimal sw match/mismatch scores, and improves speed by letting us reverse shm V indels before running sw (so fewer sw iterations; also vsearch is faster than sw). Setting this option skips the preliminary vsearch step (although if caching parameters, we still use vsearch to remove less-likely genes; but no vsearch info is passed to sw)')

# ----------------------------------------------------------------------------------------
subconfig = {
    'version'          : {'func' : int, 'help' : 'print version information and exit'},  # int doesn't do anything, it's just because I have to put something here
    'cache-parameters' : {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'},
    'annotate'         : {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'},
    'partition'        : {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'},
    'simulate'         : {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'},
    'view-output'      : {'func' : run_partitiondriver, 'help' : 'Print partitions and/or annotations from an existing output yaml file.'},
    'view-annotations' : {'func' : run_partitiondriver, 'help' : 'Print annotations from an existing annotation output csv.'},
    'view-partitions'  : {'func' : run_partitiondriver, 'help' : 'Print partitions from an existing partition output csv.'},
    'view-alternative-naive-seqs' : {'func' : run_partitiondriver, 'help' : 'Print (to std out) a comparison of the naive sequences and v/d/j gene calls corresponding to sub- and super-clusters of the cluster specified with --queries. You must have specified --calculate-alternative-naive-seqs (or --persistent-cachefname) in a previous partition step so that this information was saved.'},
    'plot-partitions'  : {'func' : run_partitiondriver, 'help' : 'Plot existing partitions and cluster annotations.'},
    'get-tree-metrics' : {'func' : run_partitiondriver, 'help' : 'Calculate tree-based selection metrics on existing output.'},
    'run-viterbi'      : {'func' : run_partitiondriver, 'help' : 'deprecated, do not use'},
}

def runs_on_existing_output(actionstr):
    if actionstr == 'get-tree-metrics':
        return True
    return actionstr.split('-')[0] in ['view', 'plot']

subargs = {subname : [] for subname in subconfig}

# ----------------------------------------------------------------------------------------
subargs['annotate'].append({'name' : '--get-tree-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster.'}})

# ----------------------------------------------------------------------------------------
subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Throw out all sequences that are not clonally related to this sequence id. Much much much faster than partitioning the entire sample (well, unless your whole sample is one family).'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same effect as --seed-unique-id, but specifies the sequence instead of that sequence\'s id (so that it doesn\'t have to be in the original input file)'}})
subargs['partition'].append({'name' : '--random-seed-seq', 'kwargs' : {'action' : 'store_true', 'help' : 'choose a sequence at random from the input file, and use it as the seed for seed partitioning (as if it had been set as the --seed-unique-id)'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based (e.g. vollmers) clustering'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 10, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})
subargs['partition'].append({'name' : '--small-clusters-to-ignore', 'kwargs' : {'help' : 'colon-separated list (or dash-separated inclusive range) of cluster sizes to throw out after several partition steps. E.g. \'1:2\' will, after <--n-steps-at-which-to-ignore-small-clusters> partition steps, throw out all singletons and pairs. Alternatively, \'1-10\' will ignore all clusters with size less than 11.'}})
subargs['partition'].append({'name' : '--n-steps-after-which-to-ignore-small-clusters', 'kwargs' : {'type' : int, 'default' : 3, 'help' : 'number of partition steps after which to throw out small clusters (where "small" is controlled by <--small-clusters-to-ignore>). (They\'re thrown out before this if we get to n_procs one before this).'}})
subargs['partition'].append({'name' : '--n-final-clusters', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and there are still more than this many clusters, attempt to keep merging until there aren\'t. NOTE should\'ve called this --n-max-final-clusters, but too late to change now'}})
subargs['partition'].append({'name' : '--min-largest-cluster-size', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and the largest cluster isn\'t this big, attempt to keep merging until it is.'}})
subargs['partition'].append({'name' : '--calculate-alternative-naive-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'write to disk all the information necessary to, in a later step (\'view-alternative-naive-seqs\'), print alternative inferred naive sequences (i.e. visualize uncertainty in the inferred naive sequence). This is largely equivalent to setting --write-additional-cluster-annotations to \'sys.max_int:sys.max_int\'.'}})
subargs['partition'].append({'name' : '--max-cluster-size', 'kwargs' : {'type' : int, 'help' : 'stop clustering immediately if any cluster grows larger than this (useful for limiting memory usage, which can become a problem when the final partition contains very large clusters)'}})
subargs['partition'].append({'name' : '--write-additional-cluster-annotations', 'kwargs' : {'help' : 'in addition to writing annotations for each cluster in the best partition, also write annotations for all the clusters in several partitions on either side of the best partition. Specified as a pair of numbers \'m:n\' for m partitions before, and n partitions after, the best partition.'}})
subargs['partition'].append({'name' : '--get-tree-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster.'}})

# ----------------------------------------------------------------------------------------
# basic simulation:
subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--mimic-data-read-length', 'kwargs' : {'action' : 'store_true', 'help' : 'trim V 5\' and D 3\' to mimic read lengths seen in data'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter describing the number of leaves per tree (maybe the mean, maybe not -- depends on the distribution)'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'distribution from which to draw the number of leaves for each tree'}})
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})  # NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--root-mrca-weibull-parameter', 'kwargs' : {'type' : float, 'help' : 'if set, uses TreeSimGM (instead of TreeSim), and value passed as the parameter (e.g. 0.1: long root-mrca distance, lots of shared mutation; 5: short, little) NOTE requires installation of TreeSimGM'}})
subargs['simulate'].append({'name' : '--input-simulation-treefname', 'kwargs' : {'help' : 'file with newick-formatted lines corresponding to trees to use for simulation. Note that a) the tree depths are rescaled according to the shm rates requested by other command line arguments, i.e. the depths in the tree file are ignored, and b) the resulting sequences do not use the leaf names from the trees.'}})
# shm indels:
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with SHM indels'}})
subargs['simulate'].append({'name' : '--n-indels-per-indeld-seq', 'kwargs' : {'default' : '1:2', 'help' : 'list of integers from which to choose the number of SHM indels in each sequence that we\'ve already decided has indels'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'type' : float, 'default' : 5, 'help' : 'mean length of each SHM indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'choices' : [None, 'v', 'cdr3'], 'help' : 'where to put the SHM indels. All options exclude the first and last five bases of the bcr sequence (None: anywhere in sequence, v: before cysteine, cdr3: within cdr3'}})
# from-scratch (rather than mimicking a particular data sample):
subargs['simulate'].append({'name' : '--rearrange-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for rearrangement-level parameters, and instead make up some plausible stuff from scratch. Have to also set --shm-parameter-dir.'}})
subargs['simulate'].append({'name' : '--mutate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for shm-level (mutation) parameters, and instead make up stuff from scratch (by default this means shm rate varies over positions and sequences, but is the same for all regions). Have to also set --reco-parameter-dir.'}})
subargs['simulate'].append({'name' : '--simulate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'same as setting both --rearrange-from-scratch and --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--shm-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve shm-level info when --rearrange-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--reco-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve rearrangement-level info when --mutate-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq', 'kwargs' : {'type' : float, 'default' : 0.05, 'help' : 'shm rate used by --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--flat-mute-freq', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all positions (in practice it\'s not that much flatter than the Gamma that is used by default --mutate-from-scratch). For use with --mutate-from-scratch.'}})
subargs['simulate'].append({'name' : '--same-mute-freq-for-all-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all sequences. For use with --mutate-from-scratch. NOTE: this means we tell bppseqgen to use the same expected rate for every sequence -- there\'s still variance in the resulting number of output mutation per sequence.'}})
# new allele/germline set generation (this is also for from-scratch)
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a subset of the available genes to represent this sample\'s germline.'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'default' : glutils.default_n_genes_per_region, 'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set)'}})
subargs['simulate'].append({'name' : '--n-sim-alleles-per-gene', 'kwargs' : {'default' : glutils.default_n_alleles_per_gene, 'help' : 'colon-separated list of mean alleles per gene for each region (for use with --generate-germline-set).'}})
subargs['simulate'].append({'name' : '--min-sim-allele-prevalence-freq', 'kwargs' : {'default' : glutils.default_min_allele_prevalence_freq,'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between 0.01 and 1'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--nsnp-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of SNPs to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})
subargs['simulate'].append({'name' : '--nindel-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of indels to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})

subargs['simulate'].append({'name' : '--subsimproc', 'kwargs' : {'action' : 'store_true', 'help' : 'Internal use only. This is set to true if this process has subsidiary simulation processes handled by Popen.'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'Internal use only. This is set to true if this is a sub-process handled by --subsimproc'}})

subargs['view-alternative-naive-seqs'].append({'name' : '--print-all-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to printing each alternative naive seq and gene call, with information on which clusters support its correctness, also print the annotations for all of these clusters below each naive sequence and gene call.'}})
subargs['view-alternative-naive-seqs'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['view-output'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)

def get_arg_names(actions):  # return set of all arg names (in the form they appear in args.__dict__) for the specified actions
    if actions == 'all':
        actions = subconfig.keys()
    return set([actionconf['name'][2:].replace('-', '_') for action in actions for actionconf in subargs[action]])

subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
args = parser.parse_args()

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for missing_arg in get_arg_names('all') - set(args.__dict__):
    args.__dict__[missing_arg] = None

if args.outfname is None and runs_on_existing_output(args.action):  # would be better to have this in processargs.py like everything else, but then I'd have to import the function somehow
    raise Exception('--outfname required for %s' % args.action)

processargs.process(args)
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
